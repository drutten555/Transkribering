{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports and requirements\n",
    "\n",
    "Code from https://github.com/m-bain/whisperX?tab=readme-ov-file. \n",
    "\n",
    "Some issues running whisperx on python 3.12.3. The following package version combinations worked:\n",
    "\n",
    "torchaudio==2.2.0, triton==2.2.0 and torch==2.2.0.\n",
    "\n",
    "This code requires Hugging Face-token.\n",
    "\n",
    "Also, as whisperx uses pyannote, permissions need to be granted to access pyannote segmentation and diarization models.\n",
    "\n",
    "Segmentation: https://huggingface.co/pyannote/segmentation-3.0\n",
    "\n",
    "Diarization: https://huggingface.co/pyannote/speaker-diarization-3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whisperx\n",
    "import json \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings\n",
    "\n",
    "Adjust e.g. batch size to 4 and compute type to int8 if low on GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" \n",
    "audioFile = os.getenv('AUDIO_PATH')\n",
    "batchSize = 16\n",
    "computeType = \"float16\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Transcription with original whisper (batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisperx.load_model(os.getenv('CONFIG'), device, compute_type=computeType) # choose large-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = whisperx.load_audio(audioFile)\n",
    "transcription = model.transcribe(audio, batch_size=batchSize)\n",
    "print(transcription[\"segments\"]) # before alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Align whisper output\n",
    "\n",
    "It's possible that the alignment model needs a different language model for Swedish audio. Should implement this from HF then:\n",
    "\n",
    "https://huggingface.co/KBLab/wav2vec2-large-voxrex-swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a, metadata = whisperx.load_align_model(language_code=transcription[\"language\"], device=device)\n",
    "wresult = whisperx.align(transcription[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "print(wresult[\"segments\"]) # after alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assign speaker labels\n",
    "Add min/max number of speakers if known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=os.getenv('HF_TOKEN'), device=device)\n",
    "diarize_segments = diarize_model(audio)\n",
    "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = whisperx.assign_word_speakers(diarize_segments, wresult)\n",
    "print(diarize_segments)\n",
    "print(result[\"segments\"]) # segments are now assigned speaker IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Export timestamps, assigned speakers and text to .json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"segments\": []\n",
    "}\n",
    "\n",
    "for seg in result['segments']:\n",
    "    formattedSegment = {\n",
    "        \"start\": seg['start'],\n",
    "        \"end\": seg['end'],\n",
    "        \"speaker\": seg['speaker'],\n",
    "        \"text\": seg['text']\n",
    "    }\n",
    "    data[\"segments\"].append(formattedSegment)\n",
    "\n",
    "with open('output.json', 'w') as jsonFile:\n",
    "    json.dump(data, jsonFile, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
