{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "import pvfalcon\n",
    "import json\n",
    "import whisper\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "AUDIO_PATH = os.getenv('AUDIO_PATH') # Path to audiofile you wish to transcribe/diarize\n",
    "JSONS_PATH = os.getenv('JSONS_PATH') # Here we save the JSONS containing the synced diarizations and transcriptions\n",
    "PICO_TOKEN = os.getenv('PICO_TOKEN') \n",
    "HF_TOKEN = os.getenv('HF_TOKEN')  \n",
    "CONFIG = os.getenv('CONFIG')\n",
    "\n",
    "def preprocess_audio(filepath):\n",
    "    '''\n",
    "    Ensures audiofile is of correct format for picavoice falcon.\n",
    "    '''\n",
    "\n",
    "    # Convert the audio file to 16 kHz mono WAV format\n",
    "    audio_path = os.getcwd()+ \"/tmp/\" + \"resampled_\" + os.path.basename(filepath)\n",
    "    if os.path.isfile(audio_path):\n",
    "        os.remove(audio_path)\n",
    "\n",
    "    # Export the audio to a temporary WAV file\n",
    "    audio = AudioSegment.from_file(filepath)\n",
    "    audio = audio.set_frame_rate(16000).set_channels(1)\n",
    "    audio.export(audio_path, format=\"wav\")\n",
    "    return audio_path\n",
    "    \n",
    "def segment_score(transcript_segment, speaker_segment):\n",
    "    \"\"\"\n",
    "    Calculate the overlap score between a transcription segment and a speaker segment.\n",
    "\n",
    "    Args:\n",
    "        transcript_segment (dict): A dictionary with 'start' and 'end' keys indicating the start and end times of the transcription segment.\n",
    "        speaker_segment (dict): A dictionary with 'start' and 'end' keys indicating the start and end times of the speaker segment.\n",
    "\n",
    "    Returns:\n",
    "        float: The overlap ratio between the transcript segment and the speaker segment. The ratio is calculated as the overlap duration divided by the transcript segment duration.\n",
    "\n",
    "    Admission of guilt: This method was found in another repo and I can't for the life of me find where I got it from. Sorrrryyy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the start and end times for the transcription segment\n",
    "    transcript_segment_start = transcript_segment[\"start\"]\n",
    "    transcript_segment_end = transcript_segment[\"end\"]\n",
    "    \n",
    "    # Extract the start and end times for the speaker segment\n",
    "    speaker_segment_start = speaker_segment[\"start\"]\n",
    "    speaker_segment_end = speaker_segment[\"end\"]\n",
    "\n",
    "    # Calculate the overlap duration between the two segments\n",
    "    overlap = min(transcript_segment_end, speaker_segment_end) - max(transcript_segment_start, speaker_segment_start)\n",
    "    \n",
    "    # Calculate the duration of the transcript segment\n",
    "    transcript_duration = transcript_segment_end - transcript_segment_start\n",
    "    \n",
    "    # Calculate the overlap ratio by dividing the overlap duration by the transcript segment duration\n",
    "    overlap_ratio = overlap / transcript_duration\n",
    "    \n",
    "    return overlap_ratio\n",
    "\n",
    "def add_text_to_segments(speaker_segments_json, whisper_result):\n",
    "    \"\"\"\n",
    "    Add transcribed text from Whisper results to the corresponding speaker segments.\n",
    "\n",
    "    Args:\n",
    "        speaker_segments_json (dict): JSON object containing speaker segments.\n",
    "        whisper_result (dict): JSON object containing Whisper transcription results.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated speaker segments with transcribed text added.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the list of speaker segments and transcript segments\n",
    "    speaker_segments = speaker_segments_json['segments']\n",
    "    transcript_segments = whisper_result['segments']\n",
    "\n",
    "    # Iterate over each transcript segment\n",
    "    for t_segment in transcript_segments:\n",
    "        max_score = 0\n",
    "        best_s_segment = None\n",
    "        \n",
    "        # Find the best matching speaker segment for the current transcript segment\n",
    "        for s_segment in speaker_segments:\n",
    "            score = segment_score(t_segment, s_segment)\n",
    "            if score > max_score:\n",
    "                max_score = score\n",
    "                best_s_segment = s_segment\n",
    "\n",
    "        # Add the transcribed text to the best matching speaker segment\n",
    "        if best_s_segment:\n",
    "            if 'text' in best_s_segment:\n",
    "                best_s_segment['text'] += ' ' + t_segment['text']\n",
    "            else:\n",
    "                best_s_segment['text'] = t_segment['text']\n",
    "\n",
    "    # Return the updated speaker segments\n",
    "    return {'segments': speaker_segments}\n",
    "\n",
    "def json_add_text(json_path, whisper_transcription):\n",
    "    \"\"\"\n",
    "    Add transcribed text to a JSON file of speaker segments and save the updated JSON.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON file containing speaker segments.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open and load the JSON file containing speaker segments\n",
    "    with open(json_path, \"r\") as file:\n",
    "        speaker_segments_json = json.load(file)\n",
    "\n",
    "    # Add transcribed text to the speaker segments using the provided Whisper transcription\n",
    "    updated_segments_json = add_text_to_segments(speaker_segments_json, whisper_transcription)\n",
    "    \n",
    "    # Save the updated speaker segments to a new JSON file\n",
    "    with open(json_path, \"w\") as file:\n",
    "        json.dump(updated_segments_json, file, indent=4)\n",
    "\n",
    "    # Inform the user that the updated JSON file was created successfully\n",
    "    print(\"Updated JSON file created successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcribe using whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Whisper model for transcription\n",
    "whisper_model = whisper.load_model(CONFIG) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe audio clip using whisper\n",
    "whisper_transcription = whisper_model.transcribe(AUDIO_PATH,\n",
    "                                                temperature = 0.2,\n",
    "                                                beam_size = 10,\n",
    "                                                best_of = 2,\n",
    "                                                no_speech_threshold = 0.3,\n",
    "                                                initial_prompt='A training sales call. Michael, the boss, oversees Dwights practice call to his colleague Jim who uses the pseudonym “William (Bill) M. Buttlicker”. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diarize using falcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "falcon_model = pvfalcon.create(access_key=PICO_TOKEN)  # Creating Falcon model with Picovoice token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON file created successfully.\n",
      "Falcon diarization JSON saved to /home/filbern/P3/JSONS/falcon_diarization.json\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the audio file to ensure correct sampling rate and format\n",
    "tmp_audio_path = preprocess_audio(AUDIO_PATH)\n",
    "\n",
    "# Perform diarization on the preprocessed audio file\n",
    "falcon_diarization = falcon_model.process_file(tmp_audio_path)\n",
    "\n",
    "# Remove the temporary audio file after processing\n",
    "os.remove(tmp_audio_path)\n",
    "\n",
    "# Extract diarization segments and format them into a dictionary\n",
    "falcon_segments = [\n",
    "    {\n",
    "        \"start\": segment.start_sec,\n",
    "        \"end\": segment.end_sec,\n",
    "        \"speaker\": f\"SPEAKER_{str(segment.speaker_tag).zfill(2)}\"\n",
    "    }\n",
    "    for segment in falcon_diarization\n",
    "]\n",
    "\n",
    "\n",
    "# THE FOLLOWING CODE IS IDENTICAL FOR PYANNOTE AND FALCON\n",
    "# Create a JSON object with the extracted segments\n",
    "falcon_json = {\"segments\": falcon_segments}\n",
    "\n",
    "# Define the path to save the JSON file\n",
    "falcon_json_path = os.path.join(JSONS_PATH, \"falcon_diarization.json\")\n",
    "\n",
    "# Save the JSON object to a file\n",
    "with open(falcon_json_path, \"w\") as json_file:\n",
    "    json.dump(falcon_json, json_file, indent=4)\n",
    "\n",
    "# Sync the transcription and the diarization. Updates the json\n",
    "json_add_text(falcon_json_path, whisper_transcription)\n",
    "\n",
    "print(f\"Falcon diarization JSON saved to {falcon_json_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diarize using pyannote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "## Load model\n",
    "pyannote_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\", use_auth_token=HF_TOKEN)  # Loading Pyannote speaker diarization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated JSON file created successfully.\n",
      "Pyannote diarization JSON saved to /home/filbern/P3/JSONS/falcon_diarization.json\n"
     ]
    }
   ],
   "source": [
    "# Diarize using the pipeline. Specify the number of speakers\n",
    "num_speakers = 3\n",
    "pyannote_diarization = pyannote_pipeline(AUDIO_PATH, num_speakers = num_speakers)\n",
    "\n",
    "# Extract diarization segments and format them into a dictionary\n",
    "pyannote_segments = [\n",
    "    {\n",
    "        \"start\": turn.start,\n",
    "        \"end\": turn.end,\n",
    "        \"speaker\": label\n",
    "    }\n",
    "    for turn, _, label in pyannote_diarization.itertracks(yield_label=True)\n",
    "]\n",
    "\n",
    "\n",
    "# THE FOLLOWING CODE IS IDENTICAL FOR PYANNOTE AND FALCON\n",
    "# Create a JSON object with the extracted segments\n",
    "pyannote_json = {\"segments\": pyannote_segments}\n",
    "\n",
    "# Define the path to save the JSON file\n",
    "pyannote_json_path = os.path.join(JSONS_PATH, \"pyannote_diarization.json\")\n",
    "\n",
    "# Save the JSON object to a file\n",
    "with open(pyannote_json_path, \"w\") as pyannote_file:\n",
    "    json.dump(pyannote_json, pyannote_file, indent=4)\n",
    "\n",
    "# Sync the transcription and the diarization. Updates the json\n",
    "json_add_text(pyannote_json_path, whisper_transcription)    \n",
    "\n",
    "print(f\"Pyannote diarization JSON saved to {pyannote_json_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_venv",
   "language": "python",
   "name": "jupyter_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
