{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import whisper\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper model choice and load paths, initial prompt from .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(os.getenv('CONFIG')) # choose large-v2\n",
    "\n",
    "audioPath = os.getenv('AUDIO_PATH')\n",
    "textPath = os.getenv('TEXT_PATH')\n",
    "initialPrompt = os.getenv('INITIAL_PROMPT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verbose = True prints out the transcription process with time stamps and segments, easy to compare outputs of short transcriptions while adjusting parameters.\n",
    "\n",
    "Audio used; excerpt from ITeam-meeting, anknutnastiftelser.wav. \n",
    "\n",
    "Prompt used: \"En del av ett möte om dataset, Chalmers, testamentera, digitalisering av fysiska papper, use case, stiftelser, stipendier, OCR:a, skanna.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe audio\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.8,\n",
    "                              initial_prompt=initialPrompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcribe audio\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# lower temperature much better for this audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting compression ratio threshold\n",
    "Default is 2.4. Filters out segments with ratio above that. These segments are more repetitive with less coherent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase to 5, some difference but not major\n",
    "# keeps words like \"tror jag\" and the like\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# increase to 10\n",
    "# from \"Jag ser två utmaningar med den datamängden.\" (with 5)\n",
    "# to \"Så att den datamängden är ju, jag ser två utmaningar med den datamängden.\"\n",
    "# where the second is verbatim\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=10,\n",
    "                              initial_prompt=initialPrompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting logprob threshold\n",
    "Default is -1.0. Tokens with log prob below -1 are filtered out, considered a moderate threshold balancing between filtering out low-probability tokens while keeping most of the meaningful transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to 0.0, keep only highly confident predictions\n",
    "# also eliminates some \"redundant\" words like \"tror jag\" \n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              logprob_threshold = 0.0,\n",
    "                              initial_prompt=initialPrompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to -2, keeps some \"redundancy\"\n",
    "# from \"Den andra utmaningen med det är vad man kan göra med den här datan när man väl har den.\"\n",
    "# to \"Den andra utmaningen med det, det är vad man då kan göra med den här datan när man väl har den.\"\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              logprob_threshold = -2.0,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# overall for this audio not huge differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust no speech threshold\n",
    "Default is 0.6. Based on the model's confidence that speech is present in the segment. Segments with probabilities below this threshold are considered no speech.\n",
    "\n",
    "Lowering the threshold can include more background noise or faint speech. Increasing the threshold might miss quiet or unclear speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower to 0.3\n",
    "# splits into more segments, ex: segment 1: \"Så att den datamängden är ju...\", segment 2: \"Jag ser två utmaningar med den datamängden.\".\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              logprob_threshold = -1.0,\n",
    "                              no_speech_threshold = 0.3,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# seems to follow more natural speech better segment-wise\n",
    "\n",
    "# increase to 0.8\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              logprob_threshold = -1.0,\n",
    "                              no_speech_threshold = 0.8,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# as expected: trims more, fewer segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting beam size\n",
    "Default is 5. Essentially, beam size (from beam search algorithm) regulates how long the sequence candidates are before the cumulative probability is calculated and a sequence is chosen as the result, for the next step. \n",
    "\n",
    "Lowering beam size gives faster computation but likely less accuracy, and the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase to 9\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              logprob_threshold = -1.0,\n",
    "                              no_speech_threshold = 0.5,\n",
    "                              beam_size = 9,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# decrease to 2\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              logprob_threshold = -1.0,\n",
    "                              no_speech_threshold = 0.5,\n",
    "                              beam_size = 2,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# as expected, higher accuracy with larger beam size\n",
    "# beam size 9: \"Så att den datamängden är ju, jag ser två utmaningar med den datamängden.\"\n",
    "# beam size 2: \"Jag ser två utmaningar med den datamängden.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting number of \"best of\" candidates during beam search\n",
    "Default is 5. Determines the number of sequence candidates considered when choosing the best possible sequence in beam search. \n",
    "\n",
    "Default is usually enough but for higher accuracy, with computational resources, it can be increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting patience during beam search\n",
    "Default is 1.0. Patience controls how many sequence candidates are considered when calculating cumulative probabilities in order to select the best sequence. The parameter is multiplicative; set to 2.0 will double the number of candidates considered compared to default.\n",
    "\n",
    "Increased patience can lead to higher transcription quality, but the computational requirements need to be considered, especially for multiple, or long, audiofiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              logprob_threshold = -1.0,\n",
    "                              no_speech_threshold = 0.5,\n",
    "                              beam_size = 5,\n",
    "                              patience = 1.0,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# double default\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              logprob_threshold = -1.0,\n",
    "                              no_speech_threshold = 0.5,\n",
    "                              beam_size = 5,\n",
    "                              patience = 2.0,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# for this audio, not a large difference, some spelling variations of unusual words, slightly more verbatim with increased patience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting length penalty\n",
    "Default is none, range [0,1]. Adjusts balance between length of transcribed output and its probability.\n",
    "\n",
    "Higher value encourages model to have more detailed and comprehensive transcriptions even if they are longer. Lets the model favor longer transcriptions.\n",
    "\n",
    "Lower value is more appropriate for generating summaries or key points from the audio, eliminates filler words.\n",
    "\n",
    "The default value is neutral, meaning that the model doesn't favor shorter nor longer outputs and only relies on the probability of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase to 0.2\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              logprob_threshold = -1.0,\n",
    "                              no_speech_threshold = 0.5,\n",
    "                              beam_size = 5,\n",
    "                              patience = 1.0,\n",
    "                              length_penalty = 0.2,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# increase to 0.8\n",
    "result = model.transcribe(audioPath, fp16=False,\n",
    "                              task=\"transcribe\",\n",
    "                              verbose = True,\n",
    "                              temperature=0.4,\n",
    "                              compression_ratio_threshold=5,\n",
    "                              logprob_threshold = -1.0,\n",
    "                              no_speech_threshold = 0.5,\n",
    "                              beam_size = 5,\n",
    "                              patience = 1.0,\n",
    "                              length_penalty = 0.8,\n",
    "                              initial_prompt=initialPrompt)\n",
    "\n",
    "# for this file, no differences really"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppress tokens\n",
    "Default is none. Can be used to avoid transcribing common noise words or specific phrases like \"uh\" or \"um\", or control punctuation by \".\", \",\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefix\n",
    "Default is none. A string that appears at the beginning of the transcription, e.g. \"This is a meeting about\" when the audio starts with \"new guidelines\". Mainly useful for starting a transcription in a specific context such as for audio chunks of meetings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
